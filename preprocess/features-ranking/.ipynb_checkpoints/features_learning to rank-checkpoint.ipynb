{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import gensim\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import glob\n",
    "import gzip\n",
    "import pickle\n",
    "from timeit import default_timer as timer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from datetime import datetime, timedelta\n",
    "from nltk import distance \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import fnmatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import the background set, and put it to a pandas data frame. \n",
    "\"\"\"\n",
    "\n",
    "root_dir  = os.path.abspath('..')\n",
    "bg_set_dir = root_dir + '/data/background_set/'\n",
    "dfs = []\n",
    "\n",
    "\"\"\"\n",
    "Load al the 10 files separetly, then append them to one df\n",
    "\"\"\"\n",
    "\n",
    "def make_data_frame(set_dir):\n",
    "    for file in os.listdir(set_dir):\n",
    "        if fnmatch.fnmatch(file, '*.out'):\n",
    "            file_path =  set_dir + '/' + file\n",
    "            dfs.append(pd.read_csv(file_path, delim_whitespace=True))\n",
    "\n",
    "    df = dfs[0] # start\n",
    "   \n",
    "    for i in range(1,len(dfs)):\n",
    "        df.append(dfs[i])\n",
    "\n",
    "    # add col names\n",
    "    df.columns = [\"SessionId\", \"AnonID\", \"Query\"]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "bg_df = make_data_frame(root_dir + '/data/background_set/')\n",
    "train_df = make_data_frame(root_dir + '/data/training_set/')\n",
    "test_df = make_data_frame(root_dir + '/data/test_set/')\n",
    "val_df = make_data_frame(root_dir + '/data/validation_set/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Query  SessionID  Label\n",
      "0           1482,244      23422      1\n",
      "1  1482,244,1647,926      23422      0\n",
      "2          12374,409      23422      0\n",
      "3            213,244      23422      0\n",
      "4              2,0,1      23422      0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Query</th>\n",
       "      <th>SessionID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1482,244</td>\n",
       "      <td>23422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1482,244,1647,926</td>\n",
       "      <td>23422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12374,409</td>\n",
       "      <td>23422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>213,244</td>\n",
       "      <td>23422.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2,0,1</td>\n",
       "      <td>23422.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label              Query  SessionID\n",
       "0    1.0           1482,244    23422.0\n",
       "1    0.0  1482,244,1647,926    23422.0\n",
       "2    0.0          12374,409    23422.0\n",
       "3    0.0            213,244    23422.0\n",
       "4    0.0              2,0,1    23422.0"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demo for the apply function I made below\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "s_id = 23422\n",
    "query = '1482,244'\n",
    "\n",
    "s_size = 4\n",
    "# select the last query from a session\n",
    "anchor_query =  bg_df[bg_df['SessionId'] == s_id].tail(1)\n",
    "\n",
    "query_string =  anchor_query.iloc[0]['Query']\n",
    "session_id = anchor_query.iloc[0]['SessionId']\n",
    "\n",
    "\n",
    "\n",
    "# get all the session ID's where this query is occuring in\n",
    "session_ids = bg_df.loc[bg_df['Query'] ==  query_string]['SessionId']\n",
    "\n",
    "# make a df with all the sessions selected by the session ID's generated above\n",
    "sessions = bg_df.loc[bg_df['SessionId'].isin(session_ids)]\n",
    "\n",
    "# filter out the anocher query\n",
    "sessions = sessions.loc[sessions['Query'] !=  query_string]\n",
    "\n",
    "# get the total amount of queries over all the selected sessions \n",
    "amount_queries = sessions.shape[0]\n",
    "\n",
    "# get the ferquentie how many time a query co-occur with the anoch query ordered\n",
    "co_accur = sessions['Query'].value_counts()/ amount_queries\n",
    "if co_accur.size > s_size:\n",
    "    suggestions = co_accur.index.values\n",
    "\n",
    "\n",
    "queries = [query_string]\n",
    "queries.extend(suggestions[0:s_size]) \n",
    "session_id = [session_id  for i in range(len(queries))]\n",
    "\n",
    "\n",
    "d = {\"SessionID\":session_id,\"Query\":queries}\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df['Label'] = 0\n",
    "df.loc[df['Query'] ==  query_string, ['Label']] = 1\n",
    "\n",
    "\n",
    "df_empty = pd.DataFrame({'SessionID' : [], 'Query': [] , 'Label': []})\n",
    "df_empty.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_query_suggestions(session, bg_data, datat_frame, s_size=19):\n",
    "    # select the last query from a session\n",
    "   \n",
    "    anchor_query =  session.tail(1)\n",
    "    \n",
    "    query_string =  anchor_query.iloc[0]['Query']\n",
    "    session_id = anchor_query.iloc[0]['SessionId']\n",
    "    \n",
    "    # get all the session ID's where this query is occuring in\n",
    "    session_ids = bg_data.loc[bg_data['Query'] ==  query_string]['SessionId']\n",
    "    \n",
    "    # make a df with all the sessions selected by the session ID's generated above\n",
    "    sessions = bg_data.loc[bg_data['SessionId'].isin(session_ids)]\n",
    "    \n",
    "    # filter out the anocher query\n",
    "    sessions = sessions.loc[sessions['Query'] !=  query_string]\n",
    "    \n",
    "    # get the total amount of queries over all the selected sessions \n",
    "    amount_queries = sessions.shape[0]\n",
    "    \n",
    "    co_accur = sessions['Query'].value_counts()/ amount_queries\n",
    "    if co_accur.size > s_size:\n",
    "        suggestions = co_accur.index.values\n",
    "    \n",
    "        queries = [query_string]\n",
    "        queries.extend(suggestions[0:s_size]) \n",
    "        session_id = [session_id  for i in range(len(queries))]\n",
    "    \n",
    "        d = {\"SessionID\":session_id,\"Query\":queries}\n",
    "\n",
    "\n",
    "        df = pd.DataFrame(data=d)\n",
    "        df['Label'] = 0\n",
    "        df.loc[df['Query'] ==  query_string, ['Label']] = 1\n",
    "    \n",
    "        datat_frame.append(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "For each sessie S= {Q1,.., QM} we want to preduct the target query QM. given the context Q1,...QM-1. \n",
    "We select 20 possible candidate queries, the true querie gets a label 1. \n",
    "\n",
    "How to select the 20 candidate queries? \n",
    "\n",
    "For each session in the training, validation and test set, we extract 20 queries that most \n",
    "likely follow the anchor query in the background data, i.e. with the highest ADJ score.\n",
    "The session is included if and only if at least 20 queries have been extracted and the target query \n",
    "appears in the candidate list.\n",
    "\n",
    "\"\"\"\n",
    "query_suggestion_training = query_suggestion_test = query_suggestion_validation = pd.DataFrame({'SessionID' : [], 'Query': [] , 'Label': []})\n",
    "\n",
    "grouped_training = train_df.groupby(['SessionId'])\n",
    "test_training = test_df.groupby(['SessionId'])\n",
    "val_training = val_df.groupby(['SessionId'])\n",
    "\n",
    "grouped_training.apply(lambda session: make_query_suggestions(session, bg_df, query_suggestion_training, s_size=19))\n",
    "\n",
    "test_training.apply(lambda session: make_query_suggestions(session, bg_df, query_suggestion_test, s_size=19))\n",
    "\n",
    "val_training.apply(lambda session: make_query_suggestions(session, bg_df, query_suggestion_validation0, s_size=19))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If we have for the test, training and validation set for every valid session the query suggestions. We need to make a \n",
    "feature vector for every query\n",
    "\n",
    "\n",
    "17 features are used for ranking \n",
    "\n",
    "Candidate sollution:\n",
    "    - For each candidate suggestion, we count how many times it follows \n",
    "    the anchor query in the background data and add this count as a feature.\n",
    "    -  We use the frequency of the anchor query in the background data.\n",
    "    -  We also add the Levenshtein distance between the anchor and the suggestion.\n",
    "    - The suggestion length (charac- ters and words) and its frequency in the background set. \n",
    "    - we add 10 features corresponding to the character n-gram similarity between the suggestion and the 10 most recent queries in the context. \n",
    "    - We add the average Levenshtein distance between the suggestion and each query in the context\n",
    "    - We use the scores estimated using the context-aware Query Variable Markov Model (QVMM)\n",
    "    -  The proposed hierarchical recurrent encoder- decoder contributes one additional feature corresponding \n",
    "    to the log-likelihood of the suggestion given the context, as detailed in Section 3.4.\n",
    "\n",
    "\"\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
