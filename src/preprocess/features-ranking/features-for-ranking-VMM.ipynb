{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "from collections import OrderedDict\n",
    "import collections\n",
    "from nltk.metrics import *\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PSTInfer(object):\n",
    "    def __init__(self):\n",
    "        self.query_to_id = {}\n",
    "        self.id_to_query = []\n",
    "\n",
    "    def _load_pickle(self, input_handle):\n",
    "        self.tuple_dict = cPickle.load(input_handle)\n",
    "        self.query_to_id = cPickle.load(input_handle)\n",
    "\n",
    "    def load(self, input_path):\n",
    "        print('Loading inference engine')\n",
    "\n",
    "        input_handle = open(input_path, 'r')\n",
    "        self._load_pickle(input_handle)\n",
    "        input_handle.close()\n",
    "        print('Preparing internal structures')\n",
    "\n",
    "        # Transform the dict of tuples to a dict of dicts\n",
    "        self.search_dict = collections.defaultdict(dict)\n",
    "        for key, freq in self.tuple_dict.items():\n",
    "            self.search_dict[key[:-1]][key[-1]] = freq\n",
    "        \n",
    "        self.tuple_dict.clear()\n",
    "        self.id_to_query = [query_str for (query_str, query_id) in \\\n",
    "                            sorted(self.query_to_id.items(), key=operator.itemgetter(1))]\n",
    "\n",
    "        print('Loaded inference engine')\n",
    "\n",
    "    def _find(self, suffix, exact_match=False):\n",
    "        _suffix = [self.query_to_id.get(x, -1) for x in suffix]\n",
    "\n",
    "        # Back off to shorter suffixes,\n",
    "        for i in range(len(_suffix)):\n",
    "            key = tuple(_suffix[i:])\n",
    "            if key in self.search_dict:\n",
    "                return {'last_node': key, \\\n",
    "                        'is_found': i==0 and len(_suffix)==len(suffix), \\\n",
    "                        'empty': False, \\\n",
    "                        'probs': self.search_dict[key]}\n",
    "        # and if nothing is found\n",
    "        return {'last_node': (0,), \\\n",
    "                'is_found': False, \\\n",
    "                'empty': True, \\\n",
    "                'probs' : {}}\n",
    "\n",
    "    def rerank(self, suffix, candidates, exact_match=False, no_normalize=False, fallback=False):\n",
    "        probs = [ self._find(suffix) ]\n",
    "        print (\"start - probs \", probs)\n",
    "        any_found = probs[0]['empty']\n",
    "        found = probs[0]['is_found']\n",
    "\n",
    "        #for i in range(len(suffix)):\n",
    "        #    probs.append(self._find(suffix[i:]))\n",
    "        # any_found = sum([p['empty'] for p in probs])\n",
    "        # Fallback to prefix matches\n",
    "        if any_found and fallback:\n",
    "            probs = []\n",
    "            last_suffix = suffix[-1].split()\n",
    "            while len(last_suffix) > 1:\n",
    "                last_suffix = last_suffix[:-1]\n",
    "                p = self._find(suffix[:-1] + [' '.join(last_suffix)])\n",
    "                if not p['empty']:\n",
    "                    probs = [ p ]\n",
    "            if len(probs) == 0:\n",
    "                print '!!!! Warning: should this be found instead ? ', suffix\n",
    "        # If we don't find anything matching the suffix\n",
    "        # we just return the original candidates\n",
    "        if exact_match and not found:\n",
    "            return [(candidate, 0) for candidate in candidates]\n",
    "        ids_candidates = map(lambda x : self.query_to_id.get(x, -1), \\\n",
    "                             candidates)\n",
    "        candidates_found = []\n",
    "        candidates_not_found = []\n",
    "        n_total_queries = len(self.id_to_query)\n",
    "\n",
    "        for (id_candidate, candidate) in zip(ids_candidates, \\\n",
    "                                             candidates):\n",
    "            # smoothed probability\n",
    "            candidate_prob = 0\n",
    "            for prob in probs:\n",
    "                if id_candidate in prob['probs']:\n",
    "                    # smooth and renormalize.\n",
    "                    if no_normalize:\n",
    "                        candidate_prob = prob['probs'][id_candidate]\n",
    "                    else:\n",
    "                        n_remaining_queries = (n_total_queries - len(prob['probs']))\n",
    "                        assert n_remaining_queries >= 0\n",
    "                        freq = prob['probs'][id_candidate]\n",
    "                        total_freq = sum(prob['probs'].values())\n",
    "                        candidate_prob = float(freq)/total_freq\n",
    "                        candidate_prob = candidate_prob/(candidate_prob\n",
    "                                        + float(n_remaining_queries)/len(self.id_to_query))\n",
    "                        candidate_prob = -np.log(candidate_prob)\n",
    "                    break\n",
    "\n",
    "            if candidate_prob == 0 and not no_normalize:\n",
    "                candidate_prob = -np.log(1.0/n_total_queries)\n",
    "            candidates_found.append((candidate, candidate_prob))\n",
    "        return zip(*candidates_found)\n",
    "\n",
    "    def suggest(self, suffix, N=100, exact_match=False):\n",
    "        result = self._find(suffix)\n",
    "\n",
    "        node = result['last_node']\n",
    "        probs = result['probs']\n",
    "\n",
    "        data = {'last_node_id' : node[0],\n",
    "                'last_node_query': self.id_to_query[node[0]],\n",
    "                'found' :   result['is_found'],\n",
    "                'suggestions' : [],\n",
    "                'scores' : []}\n",
    "        if node[0] == 0 or (exact_match and not data['found']):\n",
    "            return data\n",
    "        # Get top N\n",
    "        id_sugg_probs = sorted(probs.items(), key=operator.itemgetter(1), reverse=True)[:N]\n",
    "        string_sugg_probs = [(self.id_to_query[sugg_id], sugg_score) for sugg_id, sugg_score in id_sugg_probs]\n",
    "        sugg, score = map(list, zip(*string_sugg_probs))\n",
    "        data['suggestions'] = sugg\n",
    "        data['scores'] = score\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "class PST(object):\n",
    "    def __init__(self, D=4, q_dict=None):\n",
    "        if q_dict is None:\n",
    "            self.query_dict = {'_root_' : 0}\n",
    "        else:\n",
    "            print(\"Parsing query dictionary!\")\n",
    "            self.query_dict = q_dict\n",
    "        self.norm_dict = {}\n",
    "        self.tuple_dict = {}\n",
    "        self.normalized = False\n",
    "        self.num_nodes = 1\n",
    "        self.size = 0\n",
    "        self.D = D\n",
    "\n",
    "    def prune(self, epsilon=0.05):\n",
    "        # Transform the dict of tuples to\n",
    "        # a proper dict of dicts\n",
    "        print('Started pruning with epsilon {}'.format(epsilon))\n",
    "        search_dict = collections.defaultdict(lambda: {})\n",
    "        for key, prob in self.tuple_dict.items():\n",
    "            if not self.normalized:\n",
    "                prob = float(prob) / self.norm_dict[key[:-1]]\n",
    "            search_dict[key[:-1]][key[-1]] = prob\n",
    "        self.tuple_dict.clear()\n",
    "        logger.info('Checking constistency')\n",
    "        for key, prob in search_dict.items():\n",
    "            assert np.abs(sum(prob.values()) - 1.0) < 1e-5\n",
    "        self.normalized = True\n",
    "\n",
    "        smoothing = 1.0/len(self.query_dict)\n",
    "        logger.info('{} nodes / {} smoothing'.format(len(search_dict), smoothing))\n",
    "\n",
    "        for num, (child_key, child_probs) in enumerate(search_dict.items()):\n",
    "            if num % 100000 == 0:\n",
    "                logger.info('{} nodes explored'.format(num))\n",
    "            # The parent of 1-length contexts is the root\n",
    "            # thus we do not need to check here.\n",
    "            if len(child_key) == 1:\n",
    "                continue\n",
    "            parent_key = child_key[1:]\n",
    "            parent_probs = search_dict[parent_key]\n",
    "            kl = kl_divergence(parent_probs, child_probs, smoothing)\n",
    "            if kl <= epsilon:\n",
    "                search_dict[child_key] = {}\n",
    "        # Re-convert to tuple\n",
    "        for num, (key, probs) in enumerate(search_dict.items()):\n",
    "            for qid, qpr in probs.items():\n",
    "                join_key = key + tuple([qid])\n",
    "                assert len(join_key) >= 2\n",
    "                assert join_key not in self.tuple_dict\n",
    "                self.tuple_dict[join_key] = qpr\n",
    "        # logger.info('{} nodes - pruning done'.format(len(self.tuple_dict)))\n",
    "        self.num_nodes = len(self.tuple_dict)\n",
    "\n",
    "    def save(self, output_path, no_normalize=False):\n",
    "        print('Saving PST to {} / {} nodes.'.format(output_path, len(self.tuple_dict)))\n",
    "        # Save the normalized format\n",
    "        # if not self.normalized and not no_normalize:\n",
    "        #    logger.info('Normalizing PST')\n",
    "        #    for key, count in self.tuple_dict.iteritems():\n",
    "        #        self.tuple_dict[key] = float(count) # /self.norm_dict.get(key[:-1])\n",
    "        # self.norm_dict.clear()\n",
    "\n",
    "        f = open(output_path, 'w')\n",
    "        cPickle.dump(self.tuple_dict, f)\n",
    "        cPickle.dump(self.query_dict, f)\n",
    "        f.close()\n",
    "\n",
    "    def add_session(self, session):\n",
    "        def _update_prob(entry):\n",
    "            key = entry[:-1]\n",
    "            self.tuple_dict[entry] = self.tuple_dict.get(entry, 0) + 1\n",
    "            self.norm_dict[key] = self.norm_dict.get(key, 0) + 1\n",
    "\n",
    "        len_session = len(session)\n",
    "        if len_session < 2:\n",
    "            return\n",
    "        for query in session:\n",
    "            if query not in self.query_dict:\n",
    "                self.query_dict[query] = len(self.query_dict)\n",
    "        session = [self.query_dict[query] for query in session]\n",
    "        # print(\"add session - session \", session, str(len_session))\n",
    "        for x in range(len_session - 1):\n",
    "            tgt_indx = len_session - x - 1\n",
    "            for c in range(self.D):\n",
    "                ctx_indx = tgt_indx - c - 1\n",
    "                if ctx_indx < 0:\n",
    "                    break\n",
    "\n",
    "                entry = tuple(session[ctx_indx:tgt_indx + 1])\n",
    "                # print(\"tuple entry \", entry)\n",
    "                _update_prob(entry)\n",
    "\n",
    "                self.num_nodes = len(self.tuple_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# load the data molde\n",
    "input_handle = open('data/bg_session.ctx_ADJ.mdl', 'r')\n",
    "\n",
    "# load the tuple dict and the query dict\n",
    "tuple_dict = cPickle.load(input_handle)\n",
    "query_to_id = cPickle.load(input_handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_pickle_dict(a_dict, output_path):\n",
    "    # Save the query to ID dictionary because we need it for\n",
    "    # VMM feature construction\n",
    "    # /home/jogi/git/repository/ir2_jorg/data/query_dict.pkl\n",
    "    f = open(output_path, 'wb')\n",
    "    cPickle.dump(a_dict, f)\n",
    "# save_pickle_dict(query_to_id, '/home/jogi/git/repository/ir2_jorg/data/query_dict.pkl')\n",
    "# d = cPickle.load(open('/home/jogi/git/repository/ir2_jorg/data/query_dict.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a inverted version of the query to id dict\n",
    "id_to_query =  {v: k for k, v in query_to_id.iteritems()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When using enumerate you can only use this ones for the data set, you need to reload the data\n",
    "before you can use emenumerate again\n",
    "\"\"\"\n",
    "def open_data():\n",
    "    val_sessions = open('data/val_session.ctx', 'r')\n",
    "    train_session = open('data/tr_session.ctx', 'r')\n",
    "    bg_session = open('data/bg_session.ctx', 'r')\n",
    "    \n",
    "    return train_session, val_sessions, bg_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use the keys (tuples with two query id's) of the tuple dict to make a new dict \n",
    "tuple_pairs = tuple_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "search_dict = collections.defaultdict(dict)\n",
    "\"\"\"\n",
    "make a new dict with key anchor query, as value we have a new dict with keys previous query and \n",
    "their value count \n",
    "\n",
    "dict[anchor_query] = { previous_query: count_value}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "for _tuple in tuple_pairs:\n",
    "    search_dict[_tuple[1]][_tuple[0]] = tuple_dict[_tuple] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Func to print the suggested query id's as strings using the id_to_query map\n",
    "\"\"\"\n",
    "def print_suggestion(suggestions):\n",
    "    for suggest in suggestions:\n",
    "        print id_to_query[suggest[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that makes suggestions for a session\n",
    "\n",
    "Input: session file, *.ctx\n",
    "Output: dict with key:session_idx value: (target_query,anchor_query, session, suggestions)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def make_suggestions(session_file, recent_queries=1,num_suggestions=20):\n",
    "    # make a dict to save all the results\n",
    "    suggestion_dict = {}\n",
    "    c = 1\n",
    "    # loop over every session in the *.ctx file\n",
    "    for idx, line in enumerate(session_file):\n",
    "        # queries are tab-separated \n",
    "        session = line.strip().split('\\t')\n",
    "        \n",
    "        if len(session) >= recent_queries+1:\n",
    "            target_query = session[-1] # target query is the last query Qm\n",
    "            anchor_query = session[-2] # Anchor query is the query Qm-1\n",
    "            context = session[:-1] # Qm-1 till Q1 are the context queries\n",
    "            \n",
    "            # find anchor in the background set\n",
    "            if anchor_query in query_to_id:\n",
    "                key =  query_to_id[anchor_query] # the key of the query in the bg-set \n",
    "                # check if target query and anchor query are in the background set\n",
    "                if key in search_dict and target_query in query_to_id:\n",
    "                    \"\"\"\n",
    "                    We could use the search dict to find all the queries that follow the anchor query \n",
    "                    in the bg set, we use this queries as suggestions\n",
    "                    \"\"\"\n",
    "                    suggestions = search_dict[key]\n",
    "                    if len(suggestions) > num_suggestions: # we need at least 20 suggestions \n",
    "                        target_key = query_to_id[target_query] # find the key of the target query\n",
    "                        list_suggestions = [(key, suggestions[key] )for key in suggestions.keys()]\n",
    "                        # sort list of tuples by second tuple entry which is the frequency count\n",
    "                        # also reverse order so it is in descending order\n",
    "                        sorted_suggestions = sorted(list_suggestions, key=lambda x: x[1])[::-1]\n",
    "                        #take only the top 20 suggestions based on counts \n",
    "                        suggestions = sorted_suggestions[0:num_suggestions]\n",
    "                        # final check, is the target query really in the set of suggestions? \n",
    "                        if target_key in (x[0] for x in suggestions): \n",
    "                            # we have a valid session, now we list all the suggestions and sort them\n",
    "                            # save this in the dict key(idx):(target_query,anchor_query, session, suggestions)\n",
    "                            suggestion_dict[idx] = (target_query,anchor_query, session, suggestions)\n",
    "    return suggestion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_session, val_sessions, bg_session = open_data() # reload the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24332\n"
     ]
    }
   ],
   "source": [
    "# dicts with results\n",
    "suggestion_train = make_suggestions(train_session, recent_queries=5)\n",
    "save_pickle_dict(suggestion_train, '/home/jogi/git/repository/ir2_jorg/baselines/tests/tr_suggest.pkl')\n",
    "print len(suggestion_train)\n",
    "# suggestion_val = make_suggestions(val_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input: session file with string queries\n",
    "Output: dict with the query frequencies \n",
    "\"\"\"\n",
    "def make_query_frequncies(session_file):\n",
    "    query_freq = {}\n",
    "    total_freq = 0\n",
    "    for num, session in enumerate(session_file):\n",
    "        session = session.strip().split('\\t')\n",
    "        for query in session:\n",
    "            query_freq[query] = query_freq.get(query, 0.) + 1.\n",
    "            total_freq += 1\n",
    "    return query_freq\n",
    "\n",
    "query_freq = make_query_frequncies(bg_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading inference engine\n",
      "Preparing internal structures\n",
      "Loaded inference engine\n"
     ]
    }
   ],
   "source": [
    "# save the suggestions for the training set, so we can start from here later on\n",
    "suggestion_train = cPickle.load(open('../../baselines/tests/tr_suggest.pkl', 'rb'))\n",
    "\n",
    "# load the VMM model made with Allesandro's Probabilistic Suffix Tree (PST)\n",
    "# currently the context scope is limited to D=2 which means the tuple dict contains\n",
    "# tuples with max lenght of 3 (so the memory span is look 2 queries ahead)\n",
    "pstree = PSTInfer()\n",
    "pstree.load('../../baselines/tests/bg_session.ctx_VMM.mdl')\n",
    "print(\"======== READY ===========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target query  jesse mccartney\n",
      "anchor_query  jesse mccartney\n",
      "candidates  ['jesse mccartney'] 543\n",
      "candidates  ['jesse mcartney'] 0\n",
      "candidates  ['gefilte fish'] 0\n",
      "candidates  ['myspace chatrooms'] 1\n",
      "candidates  ['jensen ackles'] 1\n",
      "candidates  ['kelly clarkson'] 0\n",
      "candidates  ['jesse macartney'] 0\n",
      "candidates  ['yahoo com'] 0\n",
      "candidates  ['jessie mccartney'] 0\n",
      "candidates  ['ryan phill'] 0\n",
      "candidates  ['rieker shoes'] 0\n",
      "candidates  ['zac efron'] 3\n",
      "candidates  ['summerland'] 0\n",
      "candidates  ['jessy mcartny'] 0\n",
      "candidates  ['jesse mccartney com'] 1\n",
      "candidates  ['jesse mccarney'] 0\n",
      "candidates  ['jesse mccartnry'] 0\n",
      "candidates  ['teddy geiger'] 0\n",
      "candidates  ['taylor ball'] 0\n",
      "candidates  ['nycotb com'] 0\n",
      "target query  jesse mccartney\n",
      "anchor_query  jesse mccartney\n",
      "candidates  ['jesse mccartney'] 543\n",
      "candidates  ['jesse mcartney'] 0\n",
      "candidates  ['gefilte fish'] 0\n",
      "candidates  ['myspace chatrooms'] 1\n",
      "candidates  ['jensen ackles'] 1\n",
      "candidates  ['kelly clarkson'] 0\n",
      "candidates  ['jesse macartney'] 0\n",
      "candidates  ['yahoo com'] 0\n",
      "candidates  ['jessie mccartney'] 0\n",
      "candidates  ['ryan phill'] 0\n",
      "candidates  ['rieker shoes'] 0\n",
      "candidates  ['zac efron'] 3\n",
      "candidates  ['summerland'] 0\n",
      "candidates  ['jessy mcartny'] 0\n",
      "candidates  ['jesse mccartney com'] 1\n",
      "candidates  ['jesse mccarney'] 0\n",
      "candidates  ['jesse mccartnry'] 0\n",
      "candidates  ['teddy geiger'] 0\n",
      "candidates  ['taylor ball'] 0\n",
      "candidates  ['nycotb com'] 0\n",
      "target query  kol com\n",
      "anchor_query  kol com\n",
      "candidates  ['kol com'] 18\n",
      "candidates  ['kol'] 3\n",
      "candidates  ['nick com'] 0\n",
      "candidates  ['pbs kids com'] 0\n",
      "candidates  ['kids on line'] 0\n",
      "candidates  ['kol con'] 0\n",
      "candidates  ['mexican girls'] 0\n",
      "candidates  ['vet games'] 0\n",
      "candidates  ['kolgames com'] 0\n",
      "candidates  ['big mama house com'] 0\n",
      "candidates  ['kids kol'] 0\n",
      "candidates  ['djrick com'] 0\n",
      "candidates  ['www kol com'] 0\n",
      "candidates  ['hotmail'] 0\n",
      "candidates  ['cartoonnetwork com'] 0\n",
      "candidates  ['http kids on line com'] 0\n",
      "candidates  ['sbcglobal net'] 0\n",
      "candidates  ['kal com'] 0\n",
      "candidates  ['kolkids com'] 0\n",
      "candidates  ['www koljr com'] 0\n",
      "target query  breasts\n",
      "anchor_query  breasts\n",
      "candidates  ['breasts'] 479\n",
      "candidates  ['tits'] 1\n",
      "candidates  ['boobs'] 1\n",
      "candidates  ['breast'] 1\n",
      "candidates  ['breats'] 0\n",
      "candidates  ['sex'] 0\n",
      "candidates  ['large breasts'] 0\n",
      "candidates  ['nipples'] 2\n",
      "candidates  ['butt'] 0\n",
      "candidates  ['brests'] 0\n",
      "candidates  ['free porn'] 0\n",
      "candidates  ['dick'] 0\n",
      "candidates  ['thottbot'] 1\n",
      "candidates  ['vagina'] 3\n",
      "candidates  ['penis'] 0\n",
      "candidates  ['huge breasts'] 0\n",
      "candidates  ['nude women'] 1\n",
      "candidates  ['smut leenks com'] 1\n",
      "candidates  ['intranet intrachs mi com'] 0\n",
      "candidates  ['www mxtabs nt'] 0\n",
      "target query  langston hughes\n",
      "anchor_query  langston hughes\n",
      "candidates  ['langston hughes'] 113\n",
      "candidates  ['poetry'] 0\n",
      "candidates  ['elizabeth browning'] 0\n",
      "candidates  ['google com'] 0\n",
      "candidates  ['lagston hughes'] 1\n",
      "candidates  ['langston huges'] 0\n",
      "candidates  ['www langstonhughes'] 0\n",
      "candidates  ['walt whitman'] 0\n",
      "candidates  ['lanston hughes dream boogie'] 0\n",
      "candidates  ['pics of stock market crash'] 0\n",
      "candidates  ['sterling brown'] 0\n",
      "candidates  ['alonzo russell'] 0\n",
      "candidates  ['free cross word puzzle online makers'] 0\n",
      "candidates  ['these things lyrics she wants revenge explicit'] 0\n",
      "candidates  ['langson hughes'] 0\n",
      "candidates  ['city ne of haifa'] 0\n",
      "candidates  ['www jeanyvefdesigner com'] 0\n",
      "candidates  ['langston hughts'] 0\n",
      "candidates  ['gullah geechee'] 0\n",
      "candidates  ['pictures of ernest hemingway'] 0\n",
      "target query  ebay\n",
      "anchor_query  ebay\n",
      "candidates  ['ebay'] 5794\n",
      "candidates  ['google'] 62\n",
      "candidates  ['yahoo'] 31\n",
      "candidates  ['bay'] 9\n",
      "candidates  ['paypal'] 28\n",
      "candidates  ['craigslist'] 10\n",
      "candidates  ['my space'] 11\n",
      "candidates  ['amazon'] 24\n",
      "candidates  ['ebay com'] 45\n",
      "candidates  ['hotmail'] 4\n",
      "candidates  ['qvc'] 9\n",
      "candidates  ['myspace'] 6\n",
      "candidates  ['mapquest'] 7\n",
      "candidates  ['eba'] 3\n",
      "candidates  ['msn'] 5\n",
      "candidates  ['eb'] 2\n",
      "candidates  ['yahoo com'] 2\n",
      "candidates  ['bank of america'] 3\n",
      "candidates  ['fidelity'] 0\n",
      "candidates  ['yahoo mail'] 5\n",
      "target query  southwest airlines\n",
      "anchor_query  usair\n",
      "candidates  ['usair'] 0\n",
      "candidates  ['delta'] 0\n",
      "candidates  ['southwest airlines'] 1\n",
      "candidates  ['united airlines'] 0\n",
      "candidates  ['expedia'] 0\n",
      "candidates  ['jet blue'] 0\n",
      "candidates  ['american airlines'] 0\n",
      "candidates  ['orbitz'] 0\n",
      "candidates  ['southwest'] 0\n",
      "candidates  ['delta airlines'] 0\n",
      "candidates  ['jetblue'] 0\n",
      "candidates  ['united'] 0\n",
      "candidates  ['airtran'] 0\n",
      "candidates  ['usairway'] 0\n",
      "candidates  ['travelocity'] 0\n",
      "candidates  ['continental'] 0\n",
      "candidates  ['continental air'] 0\n",
      "candidates  ['usa3000'] 0\n",
      "candidates  ['swa'] 0\n",
      "candidates  ['spiritairlines'] 0\n",
      "target query  uhaul\n",
      "anchor_query  uhaul\n",
      "candidates  ['uhaul'] 15\n",
      "candidates  ['haul'] 1\n",
      "candidates  ['penske'] 1\n",
      "candidates  ['mapquest'] 1\n",
      "candidates  ['uhall'] 1\n",
      "candidates  ['pods'] 0\n",
      "candidates  ['budget'] 1\n",
      "candidates  ['uhaul com'] 2\n",
      "candidates  ['hertz'] 0\n",
      "candidates  ['hual'] 0\n",
      "candidates  ['hall'] 0\n",
      "candidates  ['usair'] 0\n",
      "candidates  ['haule'] 0\n",
      "candidates  ['uhul'] 0\n",
      "candidates  ['uhau'] 0\n",
      "candidates  ['enterprise'] 0\n",
      "candidates  ['enterprise rent car'] 0\n",
      "candidates  ['maps'] 0\n",
      "candidates  ['uhual'] 1\n",
      "candidates  ['rental vans'] 0\n",
      "target query  enterprise\n",
      "anchor_query  avis\n",
      "candidates  ['avis'] 3\n",
      "candidates  ['hertz'] 0\n",
      "candidates  ['budget'] 0\n",
      "candidates  ['enterprise'] 0\n",
      "candidates  ['southwest airlines'] 0\n",
      "candidates  ['alamo'] 0\n",
      "candidates  ['marriott'] 0\n",
      "candidates  ['united airlines'] 0\n",
      "candidates  ['orbitz'] 0\n",
      "candidates  ['dollar rent car'] 1\n",
      "candidates  ['enterprise rental car'] 0\n",
      "candidates  ['choice hotels'] 0\n",
      "candidates  ['usairways'] 0\n",
      "candidates  ['budget rent car'] 0\n",
      "candidates  ['expedia'] 0\n",
      "candidates  ['www orbitz cm'] 0\n",
      "candidates  ['national rental car'] 0\n",
      "candidates  ['car rentals'] 0\n",
      "candidates  ['budget car rental'] 0\n",
      "candidates  ['priceline'] 0\n",
      "target query  impetigo\n",
      "anchor_query  impetigo\n",
      "candidates  ['impetigo'] 37\n",
      "candidates  ['empitigo'] 0\n",
      "candidates  ['mupirocin'] 0\n",
      "candidates  ['impentigo'] 0\n",
      "candidates  ['bactroban'] 0\n",
      "candidates  ['medical book'] 0\n",
      "candidates  ['skin conditions'] 0\n",
      "candidates  ['impetago'] 0\n",
      "candidates  ['infintigo'] 0\n",
      "candidates  ['impetiago'] 0\n",
      "candidates  ['polymyositis'] 0\n",
      "candidates  ['eczema'] 1\n",
      "candidates  ['epistaxis'] 0\n",
      "candidates  ['impatigo'] 0\n",
      "candidates  ['dictionary'] 0\n",
      "candidates  ['impeitago in puppies'] 0\n",
      "candidates  ['treatment for impetigo'] 1\n",
      "candidates  ['impatago'] 0\n",
      "candidates  ['infantago'] 0\n",
      "candidates  ['strep'] 0\n",
      "target query  webshots\n",
      "anchor_query  webshots\n",
      "candidates  ['webshots'] 82\n",
      "candidates  ['eamc'] 0\n",
      "candidates  ['oanow com'] 0\n",
      "candidates  ['google'] 3\n",
      "candidates  ['web shots'] 0\n",
      "candidates  ['myspace'] 1\n",
      "candidates  ['webshots com'] 2\n",
      "candidates  ['hello mema4chris aol com'] 0\n",
      "candidates  ['community 20webshots'] 0\n",
      "candidates  ['barns'] 0\n",
      "candidates  ['clickbank com'] 0\n",
      "candidates  ['gimpix'] 1\n",
      "candidates  ['sleeping tushy'] 0\n",
      "candidates  ['lottery'] 0\n",
      "candidates  ['gulfcoast bank'] 0\n",
      "candidates  ['http beautiful girls com'] 0\n",
      "candidates  ['the forbidden city in beijien'] 0\n",
      "candidates  ['sumner reunion'] 0\n",
      "candidates  ['posting pictures on blogs'] 0\n",
      "candidates  ['russian translation'] 0\n",
      "target query  tires\n",
      "anchor_query  tires\n",
      "candidates  ['tires'] 257\n",
      "candidates  ['sears'] 0\n",
      "candidates  ['walmart com'] 0\n",
      "candidates  ['ires'] 0\n",
      "candidates  ['wheels'] 0\n",
      "candidates  ['pepboys'] 0\n",
      "candidates  ['ebay'] 0\n",
      "candidates  ['discount tires'] 0\n",
      "candidates  ['discount tire'] 0\n",
      "candidates  ['tire kingdom'] 0\n",
      "candidates  ['tire pros'] 2\n",
      "candidates  ['walmart'] 2\n",
      "candidates  ['goodyear'] 0\n",
      "candidates  ['auto tires'] 1\n",
      "candidates  ['fordfocustires'] 0\n",
      "candidates  ['town fair tire'] 1\n",
      "candidates  ['tirellitires'] 0\n",
      "candidates  ['lift kits'] 0\n",
      "candidates  ['hooser tires'] 0\n",
      "candidates  ['autoparts'] 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(len(pstree.search_dict))\n",
    "c = 0\n",
    "for session_key in suggestion_train.keys():\n",
    "        # tuple \n",
    "        session_tuple = suggestion_train[session_key]\n",
    "        target_query = session_tuple[0]\n",
    "        session = session_tuple[2]\n",
    "        context_queries = session_tuple[2][:-1]\n",
    "        anchor_query = session_tuple[1]\n",
    "        suggestions = session_tuple[3]\n",
    "        print \"target query \", target_query\n",
    "        print \"anchor_query \", anchor_query\n",
    "        # print \"session \", session\n",
    "        candidates = []\n",
    "        for idx, suggestion in enumerate(suggestions):\n",
    "            candidates = []\n",
    "            suggestion_id = suggestion[0]\n",
    "            candidates.append(pstree.id_to_query[suggestion_id])\n",
    "            candidates_new, scores = pstree.rerank(session, candidates, no_normalize=True, fallback=False)\n",
    "            print \"candidates \", candidates, scores[0]\n",
    "        # probs = [ pstree._find(session) ]\n",
    "        # print(\"probs \", probs)\n",
    "        # print probs[0]['empty']\n",
    "        # print probs[0]['is_found']\n",
    "        # candidates_new, scores = pstree.rerank(session, candidates, no_normalize=True, fallback=False)\n",
    "        # print \"candidates \", candidates, scores\n",
    "        if c > 10:\n",
    "            break\n",
    "        c += 1\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_letter_ngram(sentence, n=3):\n",
    "    \"\"\"\n",
    "    How many n-grams fits in this sentenec \n",
    "    \"\"\"\n",
    "    if len(sentence) < n:\n",
    "        return set(sentence)\n",
    "    local_counts = set()\n",
    "    for k in range(len(sentence.strip()) - n + 1): \n",
    "        local_counts.add(sentence[k:k+n])\n",
    "    return local_counts\n",
    "\n",
    "def matches(ng1, ng2):\n",
    "    \"\"\"\n",
    "    For both n-gram sets how many sim elements they contain\n",
    "    \"\"\"\n",
    "    return len(ng1 & ng2)\n",
    "\n",
    "def n_gram_sim(query1, query2,n=3):\n",
    "    \"\"\"\n",
    "    return n-gram similarity between two queries \n",
    "    \"\"\"\n",
    "    return matches(count_letter_ngram(query1, n), count_letter_ngram(query2, n))\n",
    "\n",
    "def make_n_gram_sim_features(context_queries,suggestion):\n",
    "    \"\"\"\n",
    "    For every suggestion make the n-gram similarity for the context queries (at most 10)\n",
    "    \"\"\"\n",
    "    n_sim = [0] * 10\n",
    "    for idx, context_query in enumerate(context_queries):\n",
    "        if idx >=10:\n",
    "            \"\"\"\n",
    "            only do this for at most 10 context queries \n",
    "            \"\"\"\n",
    "            break\n",
    "        n_sim[idx] = n_gram_sim(suggestion, context_query,n=3)\n",
    "    \n",
    "    return n_sim\n",
    "\n",
    "\n",
    "def get_VMM_score(session, suggestion, no_normalize=True, fallback=False):\n",
    "    \"\"\"\n",
    "    For every suggestion determine the VMM score (variable memory Markov score)\n",
    "    \"\"\"\n",
    "    \n",
    "    _, scores = pstree.rerank(session, suggestion, no_normalize=no_normalize, fallback=fallback)\n",
    "    \n",
    "    return scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that returens a feature vector for every suggestion \n",
    "\n",
    "Input: suggestion_dict\n",
    "Output: per session a matrix [17,20] with the feature vectors \n",
    "\"\"\"\n",
    "\n",
    "def make_suggestion_features(suggestion_dict, num_features=17):\n",
    "    global pstree\n",
    "    \n",
    "    c = 0\n",
    "    for session_key in suggestion_dict.keys():\n",
    "        # tuple \n",
    "        session_tuple = suggestion_dict[session_key]\n",
    "        target_query = session_tuple[0]\n",
    "        context_queries = session_tuple[2][:-1]\n",
    "        anchor_query = session_tuple[1]\n",
    "        suggestions = session_tuple[3]\n",
    "        VMM_scores = []\n",
    "        candidates = []\n",
    "        for idx, suggestion in enumerate(suggestions):\n",
    "            suggestion_id = suggestion[0]\n",
    "            query_string = pstree.id_to_query[suggestion_id]\n",
    "            \"\"\"\"\n",
    "            For each candidate suggestion, we count how many times it follows \n",
    "            the anchor query in the background data and add this count as a feature.\n",
    "            \"\"\"\n",
    "            follow_anchor_count = suggestion[1]\n",
    "\n",
    "            \"\"\"\n",
    "            Additionally, we use the frequency of the anchor query in the background data.\n",
    "            \"\"\"\n",
    "            # bg_freq = query_freq[query_string]\n",
    "\n",
    "            \"\"\"\n",
    "            We also add the Levenshtein distance between the anchor and the suggestion.\n",
    "            \"\"\"\n",
    "            levenshtein_distance = edit_distance(anchor_query, query_string)\n",
    "\n",
    "            \"\"\"\n",
    "            The suggestion length (characters and words)\n",
    "            \"\"\"\n",
    "            chars_leng = len(query_string) \n",
    "            word_leng = len(query_string.split())\n",
    "            \n",
    "            \n",
    "            \"\"\"\n",
    "            We add 10 features corresponding to the character n-gram similarity \n",
    "            between the suggestion and the 10 most recent queries in the context.\n",
    "            \"\"\"\n",
    "            n_gram_sim =  make_n_gram_sim_features(context_queries, suggestion)\n",
    "            \n",
    "            candidates.append(query_string)\n",
    "            VMM_scores.append(get_VMM_score(context_queries, [query_string]))\n",
    "            \n",
    "            \"\"\"\n",
    "            HRED Score\n",
    "            \"\"\"\n",
    "            hred_score = None \n",
    "            \n",
    "        if anchor_query == \"jesse mccartney\":\n",
    "            print(\"session \", context_queries)\n",
    "            print(\"anchor_query \", anchor_query)\n",
    "            print(\"candidates \", candidates)\n",
    "            print(\"VMM scores \", VMM_scores)\n",
    "            break\n",
    "        \n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('session ', ['new mexico', 'jessemccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney', 'jesse mccartney'])\n",
      "('anchor_query ', 'jesse mccartney')\n",
      "('candidates ', ['jesse mccartney', 'jesse mcartney', 'gefilte fish', 'myspace chatrooms', 'jensen ackles', 'kelly clarkson', 'jesse macartney', 'yahoo com', 'jessie mccartney', 'ryan phill', 'rieker shoes', 'zac efron', 'summerland', 'jessy mcartny', 'jesse mccartney com', 'jesse mccarney', 'jesse mccartnry', 'teddy geiger', 'taylor ball', 'nycotb com'])\n",
      "('VMM scores ', [543, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "make_suggestion_features(suggestion_train, num_features=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
