{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cPickle\n",
    "from collections import OrderedDict\n",
    "import collections\n",
    "from nltk.metrics import *\n",
    "import operator\n",
    "import os\n",
    "from PST_engine import PSTInfer, PST\n",
    "\"\"\"\n",
    "    we assume this ipython notebook resides in the following git-repo directory structure:\n",
    "    ir2\n",
    "    |---src\n",
    "        |----preprocessing\n",
    "                ipython notebook\n",
    "        |----data\n",
    "             | -- bg_session.ctx\n",
    "                  tr_session.ctx\n",
    "                  ...\n",
    "        |----baseline (directory of Allesandro programs)\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "DATA_PATH = '../data/'\n",
    "# raw session files, query words, tab separated queries, space separated words, one line=one session\n",
    "bg_session_filename = os.path.join(DATA_PATH, 'bg_session.ctx')\n",
    "val_session_filename = os.path.join(DATA_PATH,'val_session.ctx')\n",
    "test_session_filename = os.path.join(DATA_PATH,'test_session.ctx')\n",
    "tr_session_filename = os.path.join(DATA_PATH,'tr_session.ctx')\n",
    "# query frequency dict of the background data\n",
    "bg_query_freq_file = os.path.join(DATA_PATH, 'bg_query_freq.pkl')\n",
    "# ADJ model filename of background data (generated with Allesandro programs)\n",
    "bg_ADJ_model_filename = os.path.join(DATA_PATH, 'bg_session.ctx_ADJ.mdl')\n",
    "# VMM model filename of background data (generated with Allesandro programs)\n",
    "VMM_model_file = os.path.join(DATA_PATH, 'bg_session.ctx_VMM.mdl')\n",
    "# after loading it once, we can save it to pickle and load pickle file next time...which is quicker\n",
    "VMM_model_pickle = os.path.join(DATA_PATH, 'bg_pstreeVMM.pkl')\n",
    "# candidate filename, used to store the dict that holds the sessions & candidate queries for test/val/train\n",
    "tr_sess_candid_f = os.path.join(DATA_PATH, 'tr_suggest.pkl')\n",
    "val_sess_candid_f = os.path.join(DATA_PATH, 'val_suggest.pkl')\n",
    "test_sess_candid_f = os.path.join(DATA_PATH, 'test_suggest.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully made background frequency dictionary\n",
      "Successfully saved dict to ../data/bg_query_freq.pkl\n",
      "Start making search dict...\n",
      "Successfully made search dict\n",
      " ---------->>> READY let's start <<<-----------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Func to print the suggested query id's as strings using the id_to_query map\n",
    "\"\"\"\n",
    "def print_suggestion(suggestions):\n",
    "    for suggest in suggestions:\n",
    "        print id_to_query[suggest[0]]\n",
    "        \n",
    "        \n",
    "\"\"\"\n",
    "    Save a dictionary to file\n",
    "\"\"\"\n",
    "def save_pickle_dict(a_dict, output_file):\n",
    "    f = open(output_file, 'wb')\n",
    "    cPickle.dump(a_dict, f)\n",
    "    print(\"Successfully saved dict to %s\" % output_file)\n",
    "    f.close()\n",
    "\n",
    "\"\"\"\n",
    "    make a inverted version of the query to id dict\n",
    "\"\"\" \n",
    "def make_id_to_query_dict(q_to_id_dict):\n",
    "    return {v: k for k, v in q_to_id_dict.iteritems()}\n",
    "\n",
    "\"\"\"\n",
    "Make a query frequency dictionary of the background data set\n",
    "we need this dict for one of the features: \n",
    "    -- the frequency of an anchor query in the background data set\n",
    "Input: session file with string queries\n",
    "Output: dict with the query frequencies \n",
    "\"\"\"\n",
    "def make_query_frequencies(session_file):\n",
    "    query_freq = {}\n",
    "    total_freq = 0\n",
    "    for num, session in enumerate(session_file):\n",
    "        session = session.strip().split('\\t')\n",
    "        for query in session:\n",
    "            query_freq[query] = query_freq.get(query, 0.) + 1.\n",
    "            total_freq += 1\n",
    "    print(\"Successfully made background frequency dictionary\")\n",
    "    return query_freq\n",
    "\n",
    "\"\"\"\n",
    "    load the 2 dicts from the ADJ model that we generated with Allesandro programs\n",
    "    we need these dicts to generate the candidate queries for a test/train/val files\n",
    "\"\"\"\n",
    "def load_ADJ_model_dicts(filename):\n",
    "    print(\"Loading ADJ model from file %\" % filename)\n",
    "    input_handle = open(filename, 'r')\n",
    "    tuple_dict = cPickle.load(input_handle)\n",
    "    query_to_id = cPickle.load(input_handle)\n",
    "    print(\"Successfully loaded ADJ model dicts\")\n",
    "    print(\"\\t %d entries in tuple dict\" % len(tuple_dict))\n",
    "    print(\"\\t %d entries in query_to_id dict\" % len(query_to_id))\n",
    "    \n",
    "    return tuple_dict, query_to_id\n",
    "\n",
    "\"\"\"\n",
    "make a new dict with key anchor query, as value we have a new dict with keys previous query and \n",
    "their value count \n",
    "\n",
    "dict[anchor_query] = { previous_query: count_value}\n",
    "\n",
    "\"\"\"\n",
    "def make_search_dict(tuple_dict):\n",
    "    search_dict = collections.defaultdict(dict)\n",
    "    # use the keys (tuples with two query id's) of the tuple dict to make a new dict \n",
    "    tuple_pairs = tuple_dict.keys()\n",
    "    \n",
    "    print(\"Start making search dict...\")\n",
    "    for _tuple in tuple_pairs:\n",
    "        search_dict[_tuple[1]][_tuple[0]] = tuple_dict[_tuple] \n",
    "    print(\"Successfully made search dict\")\n",
    "    del tuple_pairs\n",
    "    \n",
    "    return search_dict\n",
    "\n",
    "# ADJ_tuple_dict, query_dict = load_ADJ_model_dicts(bg_ADJ_model_filename)\n",
    "bg_query_freq = make_query_frequencies(open(bg_session_filename, 'r'))\n",
    "save_pickle_dict(bg_query_freq, bg_query_freq_file)\n",
    "\n",
    "# finally make queryID to query-words dict\n",
    "id_to_query = make_id_to_query_dict(query_dict)\n",
    "search_dict = make_search_dict(ADJ_tuple_dict)\n",
    "print(\" ---------->>> READY let's start <<<-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that makes suggestions for a session\n",
    "\n",
    "Input: session file, *.ctx\n",
    "Output: dict with key:session_idx value: (target_query,anchor_query, session, suggestions)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def print_suggestions(session, anchor_query, candidates):\n",
    "    global id_to_query\n",
    "    \n",
    "    print(\"session \", session)\n",
    "    print(\"anchor_query \", anchor_query)\n",
    "    for query_1, query_2 in candidates:\n",
    "        print(\"query 1 \", id_to_query[query_1])\n",
    "        \n",
    "\n",
    "def make_suggestions(session_file, min_sess_length=1, max_sess_length=50, num_suggestions=20, early_stop=False):\n",
    "    global query_dict\n",
    "    global search_dict\n",
    "    # make a dict to save all the results\n",
    "    suggestion_dict = {}\n",
    "    c = 1\n",
    "    num_sessions = 0\n",
    "    # loop over every session in the *.ctx file\n",
    "    for idx, line in enumerate(session_file):\n",
    "        # queries are tab-separated \n",
    "        session = line.strip().split('\\t')\n",
    "        \n",
    "        # we also have to limit the session length because we can't generate a HRED log-likehood score\n",
    "        # for sessions that are too long (memory problems)\n",
    "        if len(session) >= min_sess_length+1 and len(session) <= max_sess_length:\n",
    "            target_query = session[-1] # target query is the last query Qm\n",
    "            anchor_query = session[-2] # Anchor query is the query Qm-1\n",
    "            context = session[:-1] # Qm-1 till Q1 are the context queries\n",
    "            \n",
    "            # find anchor in the background set\n",
    "            if anchor_query in query_dict:\n",
    "                key =  query_dict[anchor_query] # the key of the query in the bg-set \n",
    "                # check if target query and anchor query are in the background set\n",
    "                if key in search_dict and target_query in query_dict:\n",
    "                    \"\"\"\n",
    "                    We could use the search dict to find all the queries that follow the anchor query \n",
    "                    in the bg set, we use this queries as suggestions\n",
    "                    \"\"\"\n",
    "                    suggestions = search_dict[key]\n",
    "                    if len(suggestions) > num_suggestions: # we need at least 20 suggestions \n",
    "                        target_key = query_dict[target_query] # find the key of the target query\n",
    "                        list_suggestions = [(key, suggestions[key] )for key in suggestions.keys()]\n",
    "                        # sort list of tuples by second tuple entry which is the frequency count\n",
    "                        # also reverse order so it is in descending order\n",
    "                        sorted_suggestions = sorted(list_suggestions, key=lambda x: x[1])[::-1]\n",
    "                        #take only the top 20 suggestions based on counts \n",
    "                        suggestions = sorted_suggestions[0:num_suggestions]\n",
    "                        # final check, is the target query really in the set of suggestions? \n",
    "                        if target_key in (x[0] for x in suggestions): \n",
    "                            # we have a valid session, now we list all the suggestions and sort them\n",
    "                            # save this in the dict key(idx):(target_query,anchor_query, session, suggestions)\n",
    "                            suggestion_dict[idx] = (target_query,anchor_query, session, suggestions)\n",
    "                            # print_suggestions(session, anchor_query, suggestions)\n",
    "                            num_sessions += 1\n",
    "        if num_sessions > 3 and early_stop:\n",
    "            print(\"Break\")\n",
    "            break\n",
    "            \n",
    "    return suggestion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_suggestions(p_type=\"tr\", min_sess_length=5, max_sess_length=50, \n",
    "                         num_suggestions=20, load_existing=False, early_stop=False):\n",
    "    \"\"\"\n",
    "        find for a specific session file (train/test/val) all the corresponding\n",
    "        suggestions and store result in a dict for later processing\n",
    "        This procedures differs between experiments 4.4, 4.5, 4.6 in the paper\n",
    "    \"\"\"\n",
    "    global test_session_filename, tr_session_filename, val_session_filename\n",
    "    global tr_sess_candid_f, test_sess_candid_f, val_sess_candid_f\n",
    "    \n",
    "    if p_type == 'tr':\n",
    "        # training\n",
    "        session_file = tr_session_filename\n",
    "        output_file = tr_sess_candid_f\n",
    "    elif p_type == 'val':\n",
    "        # validation\n",
    "        session_file = val_session_filename\n",
    "        output_file = val_sess_candid_f\n",
    "    else:\n",
    "        # test sessions\n",
    "        session_file = test_session_filename\n",
    "        output_file = test_sess_candid_f\n",
    "        \n",
    "    if not load_existing:\n",
    "    \n",
    "        print(\"Generating suggestion queries for session file %s\" % session_file)\n",
    "        suggestion_dict = make_suggestions(open(session_file, 'r'), \n",
    "                                           min_sess_length=min_sess_length,\n",
    "                                           max_sess_length=max_sess_length,\n",
    "                                           num_suggestions=num_suggestions,\n",
    "                                           early_stop=early_stop)\n",
    "\n",
    "        print(\"Successfully generated suggestions for %d sessions\" % len(suggestion_dict))\n",
    "        save_pickle_dict(suggestion_dict, output_file)\n",
    "    else:\n",
    "        print(\"Loading suggestion queries from file %s\" % output_file)\n",
    "        suggestion_dict = cPickle.load(open(output_file ,'rb'))\n",
    "        print(\"Successfully loaded suggestions\")\n",
    "        \n",
    "    return suggestion_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating suggestion queries for session file ../data/tr_session.ctx\n",
      "Successfully generated suggestions for 24032 sessions\n",
      "Successfully saved dict to ../data/tr_suggest.pkl\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    we use the following output files\n",
    "    (1) training\n",
    "        tr_sess_candid_f\n",
    "    (2) validation\n",
    "        val_sess_candid_f\n",
    "        \n",
    "    (3) test\n",
    "        test_sess_candid_f\n",
    "        \n",
    "    if called with load_existing=True the dict will be loaded from an earlier saved pickle file\n",
    "\"\"\"\n",
    "suggestion_dict = generate_suggestions(p_type=\"tr\", load_existing=False, early_stop=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VMM model from pickle ../data/bg_pstreeVMM.pkl\n",
      "======== READY ===========\n"
     ]
    }
   ],
   "source": [
    "def load_VMM_model(filename, load_saved_model=False):\n",
    "    # load the VMM model made with Allesandro's Probabilistic Suffix Tree (PST)\n",
    "    # currently the context scope is limited to D=2 which means the tuple dict contains\n",
    "    # tuples with max lenght of 3 (so the memory span is look 2 queries ahead)\n",
    "    global VMM_model_pickle\n",
    "    \n",
    "    if not load_saved_model:\n",
    "        print(\"Loading VMM model from %s\" % filename)\n",
    "        print(\"Patient, this will take a while (approx 5 minutes)\")\n",
    "        pstree = PSTInfer()\n",
    "        pstree.load(filename)\n",
    "        save_pickle_dict(pstree, VMM_model_pickle)\n",
    "    else:\n",
    "        print(\"Loading VMM model from pickle %s\" % VMM_model_pickle)\n",
    "        pstree = cPickle.load(open(VMM_model_pickle, 'rb'))\n",
    "        \n",
    "    print(\"======== READY ===========\")\n",
    "    return pstree\n",
    "\n",
    "pstreeVMM = load_VMM_model(VMM_model_file, load_saved_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_letter_ngram(sentence, n=3):\n",
    "    \"\"\"\n",
    "    How many n-grams fits in this sentenec \n",
    "    \"\"\"\n",
    "    if len(sentence) < n:\n",
    "        return set(sentence)\n",
    "    local_counts = set()\n",
    "    for k in range(len(sentence.strip()) - n + 1): \n",
    "        local_counts.add(sentence[k:k+n])\n",
    "    return local_counts\n",
    "\n",
    "def matches(ng1, ng2):\n",
    "    \"\"\"\n",
    "    For both n-gram sets how many sim elements they contain\n",
    "    \"\"\"\n",
    "    return len(ng1 & ng2)\n",
    "\n",
    "def n_gram_sim(query1, query2,n=3):\n",
    "    \"\"\"\n",
    "    return n-gram similarity between two queries \n",
    "    \"\"\"\n",
    "    return matches(count_letter_ngram(query1, n), count_letter_ngram(query2, n))\n",
    "\n",
    "def make_n_gram_sim_features(context_queries,suggestion):\n",
    "    \"\"\"\n",
    "    For every suggestion make the n-gram similarity for the context queries (at most 10)\n",
    "    \"\"\"\n",
    "    n_sim = [0] * 10\n",
    "    for idx, context_query in enumerate(context_queries):\n",
    "        if idx >=10:\n",
    "            \"\"\"\n",
    "            only do this for at most 10 context queries \n",
    "            \"\"\"\n",
    "            break\n",
    "        n_sim[idx] = n_gram_sim(suggestion, context_query,n=3)\n",
    "        \n",
    "    return n_sim\n",
    "\n",
    "\n",
    "def get_VMM_score(session, suggestion, no_normalize=False, fallback=False):\n",
    "    global pstreeVMM\n",
    "    \"\"\"\n",
    "    For every suggestion determine the VMM score (variable memory Markov score)\n",
    "    \"\"\"\n",
    "    \n",
    "    _, scores = pstreeVMM.rerank(session, suggestion, no_normalize=no_normalize, fallback=fallback)\n",
    "    \n",
    "    return scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 2 output files\n",
      "../data/tr_hred_sess.ctx\n",
      "../data/tr_hred_cand.ctx\n",
      "-----> Ready <------\n"
     ]
    }
   ],
   "source": [
    "def prepare_files_hred_score(suggestion_dict, out_dir, suffix='tr'):\n",
    "    \"\"\"\n",
    "        in order to obtain the HRED scores from Allesandro's model we need to feed the score.py \n",
    "        with two input files (note raw data = the acutal words)\n",
    "        (1) sessions, tab separated queries, space separated words\n",
    "        (2) candidates belonging to that session, so on each line tab-separated the candidate queries \n",
    "        \n",
    "        the procedures generates both output files based on the input suggestion dictionary.\n",
    "        Remember, that dicionary contains per entry all necessary information:\n",
    "        \n",
    "        out_dir is just the directory where to write to\n",
    "        \n",
    "    \"\"\"\n",
    "    global pstreeVMM\n",
    "    \n",
    "    session_f = os.path.join(out_dir, suffix + \"_hred_sess.ctx\")\n",
    "    candid_f = os.path.join(out_dir, suffix + \"_hred_cand.ctx\")\n",
    "    print(\"Writing 2 output files\")\n",
    "    print(session_f)\n",
    "    print(candid_f)\n",
    "    \n",
    "    with open(session_f, 'w') as sess, open(candid_f, 'w') as cand:\n",
    "        for session_key in suggestion_dict.keys():\n",
    "            # tuple \n",
    "            session_tuple = suggestion_dict[session_key]\n",
    "            target_query = session_tuple[0]\n",
    "            context_queries = session_tuple[2][:-1]\n",
    "            anchor_query = session_tuple[1]\n",
    "            anchor_query_id = pstreeVMM.query_to_id[anchor_query]\n",
    "            suggestions = session_tuple[3]\n",
    "            queries = []\n",
    "            for idx, suggestion in enumerate(suggestions):\n",
    "                suggestion_id = suggestion[0]\n",
    "                query_words = pstreeVMM.id_to_query[suggestion_id]\n",
    "                queries.append(query_words)\n",
    "\n",
    "            sess.write(\"\\t\".join(context_queries) + \"\\n\")\n",
    "            cand.write(\"\\t\".join(queries) + \"\\n\")\n",
    "        \n",
    "    print(\"-----> Ready <------\")\n",
    "\"\"\"\n",
    "    make the files that we need to generate the HRED log-likelihood scores\n",
    "\"\"\"      \n",
    "prepare_files_hred_score(suggestion_dict, DATA_PATH, suffix='tr') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    count number of lines in a file\n",
    "\"\"\"\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "    return i + 1\n",
    "\n",
    "\"\"\"\n",
    "Function that returens a feature vector for every suggestion \n",
    "\n",
    "Input: suggestion_dict\n",
    "Output: per session a matrix [17,20] with the feature vectors \n",
    "\"\"\"\n",
    "\n",
    "def make_suggestion_features(suggestion_dict, hred_ll_file, num_features=17, do_test=False):\n",
    "    global pstreeVMM\n",
    "    global bg_query_freq\n",
    "    global query_dict\n",
    "    \n",
    "    num_of_candidates = 20\n",
    "    # Number of lines in HRED file must be equal to, -1 because of header line\n",
    "    lines_in_file = file_len(hred_ll_file)  - 1\n",
    "    expected_count = len(suggestion_dict) * num_of_candidates\n",
    "    print(\"Lines match? %d = %d\" % (lines_in_file, expected_count))\n",
    "    \n",
    "    assert lines_in_file == expected_count\n",
    "    \n",
    "    c = 0\n",
    "    \"\"\"\n",
    "        matrix_out is the final numpy matrix. The layout is as follows:\n",
    "        dim0 = number of sessions which is actually equal to the size of the suggestions dict\n",
    "               because we determined for each session from the tr, test, val session file if it passes\n",
    "               the requirements, 20 suggestions\n",
    "        dim1 = col0=anchor query ID\n",
    "               col1=suggestion/candidate query ID\n",
    "               col2-19: 18 features (the last column of these 18 is the HRED log-likelihood)\n",
    "               col20: label, which basically is zero except for the target query ID, which is one of the \n",
    "                      suggestion queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    feature_dim = num_features + 3\n",
    "    matrix_out = np.zeros((len(suggestion_dict) * 20, feature_dim))\n",
    "    session_id = 0\n",
    "    sess_less_cand = 0\n",
    "    \n",
    "    with open(hred_ll_file, 'r') as hred_ll:\n",
    "        # read header line\n",
    "        print(\"HRED-header \", hred_ll.readline() )\n",
    "        for session_key in suggestion_dict.keys():\n",
    "            # tuple \n",
    "            session_tuple = suggestion_dict[session_key]\n",
    "            target_query = session_tuple[0]\n",
    "            target_id = query_dict[target_query]\n",
    "            context_queries = session_tuple[2][:-1]\n",
    "            anchor_query = session_tuple[1]\n",
    "            anchor_query_id = pstreeVMM.query_to_id[anchor_query]\n",
    "            suggestions = session_tuple[3]\n",
    "            VMM_scores = []\n",
    "            candidates = []\n",
    "            # create an empty matrix for this session, which stores the num_of_candidates rows\n",
    "            # we're doing this because at the end of the session we will sort this matrix\n",
    "            # so that the target query (the positive candidate) is the first row in the session matrix\n",
    "            session_matrix = np.zeros((num_of_candidates, feature_dim))\n",
    "            if len(suggestions) == 20:\n",
    "                for idx, suggestion in enumerate(suggestions):\n",
    "\n",
    "                    suggestion_id = suggestion[0]\n",
    "                    query_string = pstreeVMM.id_to_query[suggestion_id]\n",
    "\n",
    "                    session_matrix[idx, 0] = anchor_query_id\n",
    "                    session_matrix[idx, 1] = suggestion_id\n",
    "                    \"\"\"\"\n",
    "                    For each candidate suggestion, we count how many times it follows \n",
    "                    the anchor query in the background data and add this count as a feature.\n",
    "                    \"\"\"\n",
    "                    follow_anchor_count = suggestion[1]\n",
    "                    session_matrix[idx, 2] = follow_anchor_count\n",
    "                    \"\"\"\n",
    "                    Additionally, we use the frequency of the anchor query in the background data.\n",
    "                    \"\"\"\n",
    "                    bg_freq = bg_query_freq[query_string]\n",
    "                    session_matrix[idx, 3] = bg_freq\n",
    "                    \"\"\"\n",
    "                    We also add the Levenshtein distance between the anchor and the suggestion.\n",
    "                    \"\"\"\n",
    "                    levenshtein_distance = edit_distance(anchor_query, query_string)\n",
    "                    session_matrix[idx, 4] = levenshtein_distance\n",
    "                    \"\"\"\n",
    "                    The suggestion length (characters and words)\n",
    "                    \"\"\"\n",
    "                    chars_leng = len(query_string) \n",
    "                    session_matrix[idx, 5] = chars_leng\n",
    "                    word_leng = len(query_string.split())\n",
    "                    session_matrix[idx, 6] = word_leng\n",
    "\n",
    "                    \"\"\"\n",
    "                    We add 10 features corresponding to the character n-gram similarity \n",
    "                    between the suggestion and the 10 most recent queries in the context.\n",
    "                    \"\"\"\n",
    "                    n_gram_sim =  make_n_gram_sim_features(context_queries, query_string)\n",
    "                    session_matrix[idx, 7:17] = n_gram_sim\n",
    "                    candidates.append(query_string)\n",
    "\n",
    "                    VMM_score = get_VMM_score(context_queries, [query_string])\n",
    "                    VMM_scores.append(VMM_score)\n",
    "                    session_matrix[idx, 17] = VMM_score\n",
    "\n",
    "                    \"\"\"\n",
    "                    HRED Score\n",
    "                    \"\"\"\n",
    "                    hred_score = float(hred_ll.readline())\n",
    "                    session_matrix[idx, 18] = hred_score\n",
    "\n",
    "                    if target_id == suggestion_id:\n",
    "                        session_matrix[idx, 19] = 1\n",
    "                    else:\n",
    "                        session_matrix[idx, 19] = 0\n",
    "\n",
    "                # ok, let's sort the session matrix first on the last column, the label (0/1) so that the target\n",
    "                # query is the first row\n",
    "                # session_matrix = session_matrix[session_matrix[:, feature_dim-1].argsort()[::-1]]\n",
    "                # let's parse the session_matrix into our final output matrix\n",
    "                start = session_id * feature_dim\n",
    "                end   = start + feature_dim\n",
    "\n",
    "                matrix_out[start:end, :] = session_matrix\n",
    "                # if anchor_query == \"jesse mccartney\":\n",
    "                    # print(\"session \", context_queries)\n",
    "                    # print(\"anchor_query \", anchor_query)\n",
    "                    # print(\"candidates \", candidates)\n",
    "                    # print(\"VMM scores \", VMM_scores)\n",
    "                    # print(matrix_out[session_id:20,feature_dim-1])\n",
    "                    # break\n",
    "                assert np.sum(matrix_out[start:end, 19] == 1) == 1, query_string + \" \" + str(target_id) + \" \" + str(anchor_query_id)\n",
    "                session_id += 1\n",
    "                if session_id % 1000 == 0:\n",
    "                    print(\"Progress, session id %d\" % session_id)\n",
    "            else:\n",
    "                sess_less_cand += 1\n",
    "                \n",
    "            if session_id > 10 and do_test:\n",
    "                break\n",
    "\n",
    "        \n",
    "    print(\"Session with less than 20 candidates %d\" % sess_less_cand)\n",
    "    return matrix_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines match? 480640 = 480640\n",
      "('HRED-header ', '0_HED_1479568981.18\\n')\n",
      "Progress, session id 1000\n",
      "Progress, session id 2000\n",
      "Progress, session id 3000\n",
      "Progress, session id 4000\n",
      "Progress, session id 5000\n",
      "Progress, session id 6000\n",
      "Progress, session id 7000\n",
      "Progress, session id 8000\n",
      "Progress, session id 9000\n",
      "Progress, session id 10000\n",
      "Progress, session id 11000\n",
      "Progress, session id 12000\n",
      "Progress, session id 13000\n",
      "Progress, session id 14000\n",
      "Progress, session id 15000\n",
      "Progress, session id 16000\n",
      "Progress, session id 17000\n",
      "Progress, session id 18000\n",
      "Progress, session id 19000\n",
      "Progress, session id 20000\n",
      "Progress, session id 21000\n",
      "Progress, session id 22000\n",
      "Progress, session id 23000\n",
      "Progress, session id 24000\n",
      "Session with less than 20 candidates 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Please note, the output files of HRED with the ll-score are called:\n",
    "    Experiments 4.4.1 (base)\n",
    "        tr_hred_score_exp4_4_1.f\n",
    "        val_hred_score_exp4_4_1.f\n",
    "        test_hred_score_exp4_4_1.f\n",
    "\"\"\"\n",
    "\n",
    "hred_ll_file = os.path.join(DATA_PATH, 'tr_hred_score_exp_4_4_1.f')\n",
    "output_matrix = make_suggestion_features(suggestion_dict, hred_ll_file, num_features=17, do_test=False)\n",
    "np.savez(os.path.join(DATA_PATH, \"train_suggest_matrix\"), output_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "        33.,  34.,  36.,  39.])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(output_matrix[:, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
